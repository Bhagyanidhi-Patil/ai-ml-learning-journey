Deep learning is a type of machine learning that uses neural networks to learn directly from raw data.
Deep learning is powerful because it can learn from unstructured data (text, images, audio) without needing hand-crafted features.

What makes deep learning special
    Traditional ML usually canâ€™t work well with raw data by itself.
    It typically needs preprocessing and feature extraction first.

    Traditional ML models (like linear regression, SVMs, decision trees):
        â€¢ Expect structured, numeric inputs
        â€¢ Donâ€™t understand raw text, images, or audio directly
        So humans have to step in.

    Deep learning learns features + prediction together, end-to-end.
        Instead of:
        â€œLetâ€™s decide what features matter, then train a modelâ€
        It does:
        â€œGive me raw data, Iâ€™ll figure out what mattersâ€
        Thatâ€™s the superpower.

What is unstructured data?
    Unstructured data doesnâ€™t come neatly in rows and columns.
    Examples:
        â€¢ Text (emails, tweets, documents)
        â€¢ Images (photos, medical scans)
        â€¢ Audio (speech, music)
        â€¢ Video
        â€¢ Raw sensor data
    Contrast that with structured data:
        â€¢ Tables, spreadsheets, databases
        â€¢ Fixed columns like age, income, clicks, labels
    Most real-world data (~80%+) is unstructured.

    How deep learning relates to unstructured data

Deep learning is especially good at unstructured data because:

    1. It learns representations automatically
    Neural networks transform raw inputs into hierarchical representations.
    Example with images:
        â€¢ Early layers â†’ edges and corners
        â€¢ Middle layers â†’ shapes and textures
        â€¢ Deep layers â†’ objects (faces, cars)
    You donâ€™t have to tell the model what an â€œedgeâ€ isâ€”it discovers that.
    2. Different architectures match different data types
    Deep learning isnâ€™t one thing; it adapts to data structure:
        â€¢ Text â†’ RNNs, Transformers (e.g., BERT, GPT)
        â€¢ Images â†’ CNNs, Vision Transformers
        â€¢ Audio â†’ CNNs + sequence models
        â€¢ Video â†’ 3D CNNs, Transformers
    These architectures exploit local patterns, sequences, and contextâ€”things unstructured data has but tables donâ€™t.
    3. It scales with data
    Unstructured data is high-dimensional and messy.
    Deep models get better as you feed them lots of data + compute, where traditional statistical models often break down.

---------------------------------------------------------------------------------------------------------------------------------
Neural networks 

    This is a neural network, made of small units called neurons (the circles).
    A neural network takes raw input, transforms it through hidden layers, and produces an output â€” learning the transformations automatically.

    Input â†’ Hidden â†’ Hidden â†’ Hidden â†’ â€¦ â†’ Output

    Theyâ€™re organized into layers:
    1. Input layer (left)
        â€¢ Labeled input
        â€¢ Each circle gets raw numbers from data
            â—‹ pixels of an image
            â—‹ word embeddings from text
            â—‹ audio signal values

    2. Hidden layer(s) (middle)
        â€¢ Labeled hidden
        â€¢ These neurons:
            â—‹ combine inputs
            â—‹ apply weights
            â—‹ pass results forward
    This is where the network learns patterns.

    3. Output layer (right)
        â€¢ Labeled output
        â€¢ Produces the final answer:
            â—‹ class label (cat / dog)
            â—‹ probability (spam or not)
    number (price, temperature, etc.)

ğŸ’¡ Training neural network = Forward Propagation + Backward Propagation
    Forward Propagation
        Forward propagation = making a prediction
        What happens:
            1. Input data goes into the network
            2. Each neuron:
                â—‹ multiplies inputs by weights
                â—‹ adds bias
                â—‹ applies an activation function
            3. The signal moves forward layer by layer
            4. The output layer produces a prediction

    Backward Propagation
        Backward propagation = learning from mistakes
        What happens:
            1. Compare the prediction with the true answer
            2. Calculate the error (loss)
            3. Send this error backward through the network
        Adjust weights and biases to reduce the error

----------------------------------------------------------------------------------------------------------------------------

Neural network architecture:
    The most basic neural network architecture is data moves in one direction only:
        Input â†’ Hidden â†’ Output
        No loops.
        No going back during prediction.

    1. Feed-Forward Neural Network (FNN):	
        â€¢ Data moves only forward
        â€¢ No memory
        â€¢ Each input is treated independently

        Example:
        â€¢ Predict house price from features
        â€¢ Classify spam using fixed inputs

        During prediction (inference)
            â€¢ âœ… Only forward propagation
            Data flows: Input â†’ Hidden â†’ Output
            â€¢ No backward flow
        This is why itâ€™s called feed-forward.

        During training
            â€¢ âœ… Forward propagation â†’ make a prediction
            â€¢ âœ… Backward propagation â†’ update weights
        So the full training loop is:
            1. Forward â†’ compute output
            2. Compute error (loss)
            3. Backward â†’ adjust weights
            4. Repeat

    2. Recurrent Neural Network (RNN):
        â€¢ Data moves forward + loops back
        â€¢ Has memory
        â€¢ Each output depends on current input + past inputs

        Example:
        â€¢ Reading a sentence word by word
        â€¢ Predicting next word
        â€¢ Stock price over time
        
        Core idea of RNN (memory)
            An RNN has a hidden state (memory).
            At each time step:
                â€¢ it takes the current input
                â€¢ combines it with the previous hidden state
                â€¢ produces:
                    â—‹ a new hidden state
                    â—‹ an output
            So it remembers past information.

        How RNN works during training

        Training has two phases.
        Phase 1: Forward propagation (through time)
            1ï¸âƒ£ Feed the sequence step by step
            2ï¸âƒ£ RNN makes predictions at each time step
            3ï¸âƒ£ Store outputs and hidden states
            4ï¸âƒ£ Compute total error (loss)
            This is called forward pass through time.

        Phase 2: Backward propagation (through time)
            1ï¸âƒ£ Error is sent backward from last time step
            2ï¸âƒ£ The network figures out:
                â€¢ which weights caused the error
                â€¢ how much each weight contributed
            3ï¸âƒ£ Weights are updated to reduce future errors
            This is called Backpropagation Through Time (BPTT).

    3. CNN (Convolutional Neural Network) is a neural network designed for images and spatial data.

        A CNN:
            1. Takes an image
            2. Extracts important visual features (edges, shapes)
            3. Uses those features to classify the image

    4. Transformers
        A Transformer is a neural network made to understand sequences (like text), but without using recurrence (no RNN loops).
        It looks at all parts of the input at once and decides what to pay attention to.

        Why Transformers were needed
        Problems with RNNs:
            â€¢ Process words one by one â†’ slow
            â€¢ Hard to remember long-range context
        Transformers fix this by:
            â€¢ Processing everything in parallel
            â€¢ Using attention instead of memory loops

        GPT is built on Transformers 
        How GPT relates to Transformers
            â€¢ Transformer â†’ the model architecture
            â€¢ GPT â†’ a specific model built using that architecture


------------------------------------------------------------------------------------------------------------------------------------
    Whatâ€™s the same in FNN and RNN 

        Both FNN and RNN are trained using:
            Forward pass â†’ compute outputs.
            Compute loss â†’ measure error.
            Backward pass â†’ compute gradients.
            Optimizer step â†’ update weights.

        So yes, the basic training mechanism is the same.

    Whatâ€™s different in FNN and RNN

        Flow of data:
            FNN: Each input is independent. No memory. Gradient only flows through layers of that input.
            RNN: Inputs are sequential. Gradient flows through layers AND through time steps (Backpropagation Through Time, BPTT).
        Hidden state / memory:
            FNN: No hidden state carried across inputs.
            RNN: Hidden state carries memory across sequence steps, and gradients must account for that memory.
        Dependencies:
            FNN: Each output depends only on that input.
            RNN: Each output can depend on all previous inputs in the sequence.

        FNN : 
        Each input is independent. You feed one data point at a time (or a batch of independent points).
        The network doesnâ€™t know anything about previous inputs.
        
        Input x â”€â”€â–º [Hidden Layer(s)] â”€â”€â–º Output y
               â”‚
               â–¼
           Loss computation
               â”‚
               â–¼
         Backpropagation
               â”‚
               â–¼
        Update weights

        RNN :
        Input is a sequence of data points.
        Each step in the sequence is fed one at a time, but the network remembers previous inputs via the hidden state.
        So the output at step t depends on current input + previous hidden state.

        x1 â”€â–º hidden1 â”€â–º y1
                â”‚
        x2 â”€â–º hidden2 â”€â–º y2
                â”‚
        x3 â”€â–º hidden3 â”€â–º y3
                â”‚
        ...   ...   ...
        xn â”€â–º hiddenn â”€â–º yn

            â–¼
            Loss computation (sum over all y)
            â”‚
            â–¼
        Backpropagation Through Time (BPTT)
            â”‚
            â–¼
            Update weights

    