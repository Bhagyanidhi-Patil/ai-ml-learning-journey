Project : "PDF Question Answering Bot (Mini RAG Project)"

This code builds a simple local RAG (Retrieval-Augmented Generation) system using:
    -> PDF as knowledge source
    -> Free embeddings (HuggingFace)
    -> FAISS vector database
    -> Local LLM via Ollama
ğŸ”¹ What is Ollama?
    Ollama is a tool/software that:
        Runs LLMs locally on your computer
        Lets you download and use models like LLaMA, Mistral, etc.
        Acts like a local LLM server

---------------------------------------------------------------------------
ğŸ”¹ What This Code Does (Big Picture)

    It allows you to:
        Load a PDF
        Break it into small chunks
        Convert those chunks into embeddings (vectors)
        Store them in a FAISS vector database
        Ask questions
        Retrieve the most relevant PDF chunks
        Send them to a local LLM (like Llama 3)
        Get an answer grounded in your PDF
        This is a fully local PDF chatbot.

---------------------------------------------------------------------------
ğŸ§© Step-by-Step Explanation :
ğŸ”¹ 1. Load PDF
    PyPDFLoader reads your PDF file.
    It extracts text from each page.
    Output: a list of Document objects (one per page).
    documents = [
        Page1 (3000 characters),
        Page2 (2500 characters),
    ]

ğŸ”¹ 2. Split Into Chunks
    Why we split:
        LLMs and embedding models work better with smaller text pieces.
    What it does:
        Splits long pages into chunks of 1000 characters
        Keeps 200 characters overlapping between chunks
    Why overlap?
        To avoid cutting sentences in half and losing context.
    docs = [
        Chunk1 (1000 chars),
        Chunk2 (1000 chars),
        Chunk3 (1000 chars),
        Chunk4 (1000 chars),
        Chunk5 (500 chars),
    ]

ğŸ”¹ 3. Create Free Embeddings
    Embedding = converting text into numbers (vectors).
    "This is AI" â†’ [0.123, -0.98, 0.456, ...]
    we are using all-MiniLM-L6-v2.

ğŸ”¹ 4. Store in FAISS
    FAISS is a vector database.
    It:
        Stores vectors
        Allows fast similarity search
    What happens here:
        Each chunk â†’ converted to embedding
        Stored inside FAISS index
    Text chunk â†’ Vector â†’ Stored in FAISS

ğŸ”¹ 5. Load Local LLM (Ollama)

ğŸ”¹ 6. Create Retriever
    This turns FAISS into a search engine.
    When you ask a question:
        It converts your question into embedding
        Finds similar document chunks
        Returns the most relevant ones

ğŸ”¹ 7. Question (Main RAG Logic)
    This creates a chatbot loop.
    ğŸŸ¢ Step A: User asks question
    ğŸŸ¢ Step B: Retrieve relevant chunks
        What happens internally:
            Question â†’ embedding
            Compare with stored vectors
            Find closest matches
            Return best chunks
    ğŸŸ¢ Step C: Combine context
        All retrieved chunks are combined into one big text block.
    ğŸŸ¢ Step D: Create Prompt
    ğŸŸ¢ Step E: Ask LLMThe 
        LLM:
        Reads context
        Reads question
        Generates answer
    ğŸŸ¢ Step F: Print Answer
---------------------------------------------------------------------------
Flow chart :

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      sample.pdf      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   PyPDFLoader       â”‚
                â”‚  (Extract text)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Text Splitter       â”‚
                â”‚ (Chunking 1000/200) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ HuggingFace         â”‚
                â”‚ Embeddings Model    â”‚
                â”‚ (Text â†’ Vectors)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ FAISS Vector Store  â”‚
                â”‚ (Store embeddings)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                           â”‚
                           â–¼
                    USER ASKS QUESTION
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Convert Question    â”‚
                â”‚ â†’ Embedding Vector  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ FAISS Similarity    â”‚
                â”‚ Search              â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Retrieve Relevant   â”‚
                â”‚ Chunks (Context)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Create Prompt       â”‚
                â”‚ (Context + Question)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Ollama (Llama3)     â”‚
                â”‚ Generate Answer     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                     FINAL ANSWER

---------------------------------------------------------------------------
ğŸ§  Why This Is Powerful
Without RAG:
LLM only knows training data.

With this:
LLM answers based on YOUR PDF.